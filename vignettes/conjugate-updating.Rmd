---
title: "Inferring listeners prior beliefs with Stan"
author: "Dave Kleinschmidt"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inferring prior beliefs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This package includes a Stan model for inferring listeners prior beliefs, based on adaptation behavior.

```{r preamble}

library(tidyverse)
library(magrittr)
library(beliefupdatr)

## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)

library(rstan)

```

# Load and format data

```{r data}

d <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  select(vot, trueCat, respCat, trial, bvotCond, subject)

```

To fit this model with stan, we need to convert the tabular data to a stan-friendly format, a list of the variables declared in teh `data` block of the stan model, which in this case looks like this:

```stan
data {
  int n;                        // number of training observations
  real x[n];                    // training observations
  int m;                        // number of categories
  int z[n];                     // categories of each training observation
  int l;                        // number of subjects
  int y[n];                     // subject labels
  int n_test;                   // number of test trials
  real x_test[n_test];          // locations of test trials
  int y_test[n_test];           // subject labels for test trials
  int z_test_counts[n_test,m];  // responses for test trials
}
```

The `prepare_data_conj_infer_prior` function automates this process.  You just need to provide training and test data, and tell it the column names for the cues, true and response categories, and groups (subjects or conditions).

```{r stan-data}

d_stan <- d %>%
  beliefupdatr::prepare_data_conj_infer_prior(training=.,
                                              test=.,
                                              cue='vot',
                                              category='trueCat',
                                              response='respCat',
                                              group='subject')

str(d_stan)

```

This is the data as it was fit originally, but there's a hidden inefficiency here: it's computing test responses separately for each subject, but using exactly the same input statistics for subjects within each condition.  So we can substantially speed things up by summarizing the data by condition instead of subject:

```{r stan-data-little}

d_stan_little <- d %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>%
  prepare_data_conj_infer_prior(training=.,
                                test=d,
                                cue='vot',
                                category='trueCat',
                                response='respCat',
                                group='bvotCond')

str(d_stan_little)

```

# Fit the model

## Getting ready

You can list the stan models included with

```{r}
beliefupdatr::list_models()
```

Stan models are pre-compiled when the package is installed.  If you want to access them directly, the compiled `stanmodel` objects are stored in the list:

```{r compile}
mod <- beliefupdatr:::stanmodels[['conj_id_lapsing_fit']]
```

These include the original Stan code, the translated C++ code, and the compiled objects.

A few tips for speeding things up: if you have multiple cores and enough memory, you can run the chains in parallel by setting

```{r cores}
options(mc.cores = parallel::detectCores())
```

Then Stan will use however many cores you have, and can run up to that many chains in parallel.  Note, though, that if you only have four cores this can tie up your laptop pretty well, so I typically use one less than the number of cores I really have (e.g., `parallel::detectCores()-1`).

## Actually fitting the model

Once you've compiled the model, you can sample from it with `rstan::sampling` call.

```{r fit-model}
fit <- rstan::sampling(mod, data=d_stan_little, chains=4, iter=500)
```

If all goes well, that shouldn't take too long.  Once you've fit the model, you can eyeball the inferred prior values with

```{r print-model}
print(fit, digits=1, pars=c('kappa_0', 'nu_0', 'mu_0', 'sigma_0', 'lapse_rate'))
```

The first thing to look for here is the `Rhat` statistic.  This is a measure of how well your chains have _mixed_, which is a way of approximating how successful your sampler was.  The idea is that if your chains are sampling from the actual posterior, then they'll mix together well.  Thus, if they _don't_ mix well, you know that something is wrong.  Of course, just because they mix well doesn't necessarily mean everything is hunky-dory, but it's a very important check.

The next thing to look at is the parameter values themselves.  Are the of the right orders of magnitude?  Are the values actually reasonable?  Can you interpret them?



# Inferring prior with incremental updating

The model above assumes that _all training data_ has been encountered before any
of the test data, and it ignores any incrementality in how listeners' behavior
changes as they update their beliefs.  Can we capture belief updating behavior
under the assumption that listeners incrementally update their beliefs, and what
to the inferred prior beliefs look like?

```{r prepare-data-inc}

# start w/ four blocks
d_stan_inc_4 <-
  d %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>%
  beliefupdatr::prepare_data_incremental_suff_stats(test=d,
                                                    cue = 'vot',
                                                    category = 'trueCat',
                                                    response = 'respCat',
                                                    group = 'bvotCond',
                                                    n_blocks = 4) %T>%
  str()

```

We have to format the data in a slightly different way here.  First, instead of
representing the training data in terms of raw observations, we're representing
it in terms of _sufficient statistics_: count, mean, and standard deviation.
The Stan code for the model above converts the raw observations into this
format, but we gain more flexibility by doing it ourselves outside of Stan in
this case.

Second, we're making a simplifying assumption that of the _sufficient
statistics_, only the count changes over blocks.  This is reasonable when
modeling aggregate data because each subject gets a random trial order, so on
average the statistics of each category will mirror the true statistics within
each block.  Specifically, we model each blocks' test observations based on the
number of tokens from each category a subject has seen, on average, up to
halfway through that block.  This is a compromise between assuming that no
updating happens throughout the block (as far as the test trials are concerned),
and that the responses reflect the updated beliefs at the _end_ of the block.

```{r fit-inc}

mod_inc <- beliefupdatr:::stanmodels[['conj_id_lapsing_sufficient_stats_fit']]

fit_inc_4 <- rstan::sampling(mod_inc,
                             data = d_stan_inc_4,
                             iter = 500, chains = 4, init=0) %T>%
  print(digits=1, pars=c('kappa_0', 'nu_0', 'mu_0', 'sigma_0', 'lapse_rate'))

```

```{r fit-inc-10}

fit_inc_10 <-
  d %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>%
  beliefupdatr::prepare_data_incremental_suff_stats(test=d,
                                                    cue = 'vot',
                                                    category = 'trueCat',
                                                    response = 'respCat',
                                                    group = 'bvotCond',
                                                    n_blocks = 10) %>%
  rstan::sampling(mod_inc,
                  data = .,
                  iter = 500, chains = 4, init = 0) %T>%
  print(digits=1, pars=c('kappa_0', 'nu_0', 'mu_0', 'sigma_0', 'lapse_rate'))
  

```
