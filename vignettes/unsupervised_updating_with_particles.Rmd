---
title: "Unsupervised belief updating with particle filters"
author: "Dave Kleinschmidt"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Why incremental unsupervised belief updating is hard

Belief updating is _supervised_ when every observation's intended category is
known.  This is a reasonable assumption in many cases (where critical stimuli
are paired with visual, lexical, or other information that unambiguously
identifies the intended category).  But in many other cases this assumption does
not make sense.  For instance, in a typical _distributional learning_ experiment
[like @Clayards2008], every word is a minimal pair (like "beach" vs. "peach").
In these cases, belief updating is _unsupervised_ (or semi-supervised).

Supervised belief updating is computationally simple: on every trial, only a
single category's beliefs need to be updated.  This naturally lends itself to
incremental updating, where the updated beliefs from trial $n$ serve as the
prior beliefs for trial $n+1$:
\[ 
p(\theta | x_{1\ldots n+1}, c_{1\ldots n+1}) 
  \propto 
  p(x_n | \theta, c_n)
  p(\theta | x_{1\ldots n}, c_{1\ldots n}) 
\]
Mathematically, unsupervised belief updating is a straightforward extension of
supervised belief updating.  Instead of conditioning on the category $c$, we
treat the category as unknown, and the updated beliefs are the result of
averaging the results of belief updating after (hypothetically) assigning the
observation to that category:
\[
p(\theta | x) 
  = \sum_i p(\theta, c=i | x)
  \propto \sum_i p(\theta | x, c=i) p(c=i)
\]
However, this makes incremental updating computationally very demanding.  If on
every trial there are two possible categories, then after $n$ trials there are
$2^n$ possible ways to categorize those trials.  To be exactly correct, we need
to average over _all_ of these hypothetical assignments, but this number quickly
becomes impossibly large: in a 200-trial experiment, there are somewhere around
$10^60$ ways to categorize all the stimuli encountered.

# Approximation strategies

While doing exact inference in these situations is often practically impossible,
there are a number of ways to approximate the exact inference.  One possibility
is a _particle filter_, where the full distribution over possible assignments of
stimuli to categories is approximated by a finite set of _particles_, each of
which represents one way of classifying the observations seen thus far.  These
particles are weighted based on how likely their particular hypothesis is given
the data seen so far.  What makes a particle filter effective is that each
particle can be updated incrementally as new data comes in, by classifying data
point $k$ based on how data points $1\ldots k-1$ were classified, and then
updating the weight based on how well that classification explains all $k$ data
points.

The filter is initialized by setting each particle based on the common prior
parameters:

```{r, particle-init}

set.seed(2)

# prior parameter to initialize particles with
p0 <- list(a = nix2_params(mu=-.5, sigma2=1, kappa=1, nu=1),
           b = nix2_params(.5, 1, 1, 1))

# initialize population of 100 particles
n_part <- 100
particles <- replicate(n_part, init_particle(p0), simplify=FALSE)

```

THere are different specific methods to fit the particle filter to data, which
differ (in this case) in how new data points are assigned classifications, and
how the population of particles is kept "fresh".[^stale-particles] The method
currently implemented is based on that of @Chen2002, as described in
@Fearnhead2004.  This method is simpler, but not as effective as that proposed
by @Fearnhead2004.

[^stale-particles]: One problem with particle filters is that the population of
particles can become "stale", with just a few particles taking most of the
weight, which vastly reduces the effective sample size of the particle filter
and hence the quality of the approximation.  The population hence needs to be
periodically refreshed, by discarding particles with very small weight and
replacing them with copies (possibly slightly modified) of particles with larger
weight.

The function `filter_chen_liu` updates the population of particles:

```{r, particle-update-one}

particles %>%
  filter_chen_liu(x=0) %>%
  filter_chen_liu(x=-1) %>%
  head(2) %>%                           # just look at 2
  str()

```

To sequentially filter multiple data points, you can use the `reduce` function
from `purrr`.  For example, this is a pretty easy problem (two clearly separated
clusters):

```{r, particle-fit}

# data to fit, a mixture with means of -4 and 4
xs <- rnorm(100) + rep(c(-4, 4), length.out=100)

ps <- reduce(xs[1:30], filter_chen_liu, .init=particles)

```

We can check to see how well the particle filter did by looking at how each
stimulus is classified by each of the particles (in this visualization, showing
the weights with opacity and ordering on the x-axis):

```{r, particle-classification}

# extract classifications under each particle and particle weights
zs <- ps %>%
  transpose() %>%
  as_data_frame() %>%
  mutate(w = as_vector(w)) %>%
  arrange(w) %>%
  mutate(particle_id = row_number())  %>%
  unnest(map(z, ~ data_frame(z=.x, x_id = 1:length(.x))))

ggplot(zs, aes(x=particle_id, y=x_id, fill=z, alpha=w)) +
  geom_tile() +
  scale_alpha_continuous(range=c(0.1, 1), trans='log10')

```

The most important factor in determining a particle's weight is _consistency_:
the weights fall of rapidly as the classififications become less consistent, as
revealed by the dominance of "stripey" classifications (since our simulated data
alternates clusters).  Additionally, by setting the prior parameters to have
slightly offset means we introduced a (very weak) identifiability constraint
(where `a` is more likely to capture the lower cluster), leading the particles
where all the low-cluster observations are `a` and the high cluster observations
are `b` to outperform the opposite mapping.

Viewed with linear opacity scaling this is even more apparent:

```{r }
ggplot(zs, aes(x=particle_id, y=x_id, fill=z, alpha=w)) +
  geom_tile() +
  scale_alpha_continuous(range=c(0.1, 1))

```


.
